# UNSUPERVISED-MACHINE-LEARNING-USING-IRIS-DATA

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df=pd.read_csv('Iris.csv')
df.head()
df.shape
df=df.drop('Id',axis=1)
df

# check is data have any null value
df.isnull().sum()
df.isna().sum()

#summary statistics
df.describe()
df.info()

#Describe the class label Find all possible labels
df['Species'].describe()
df['Species'].unique()
#plotting
df['Species'].unique()
sns.stripplot(x='Species', y='PetalLengthCm', data=df, alpha=0.3, jitter=True);
sns.stripplot(x='Species', y='PetalWidthCm', data=df, alpha=0.3, jitter=True);
sns.stripplot(x='Species', y='SepalLengthCm', data=df, alpha=0.3, jitter=True);
sns.stripplot(x='Species', y='SepalWidthCm', data=df, alpha=0.3, jitter=True);
df.plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm')
import seaborn as sns
import matplotlib.pyplot as plt
sns.FacetGrid(df,hue='Species',size=5).map(plt.scatter,'SepalLengthCm','SepalWidthCm')
plt.legend()

df.SepalLengthCm[df['Species']=='Iris-virginica']
from mpl_toolkits import mplot3d
fig = plt.figure()
ax = plt.axes(projection='3d')
ax.scatter(df.SepalLengthCm[df['Species']=='Iris-setosa'], df.SepalWidthCm[df['Species']=='Iris-setosa'], df.PetalLengthCm[df['Species']=='Iris-setosa'] )
ax.scatter(df.SepalLengthCm[df['Species']=='Iris-virginica'], df.SepalWidthCm[df['Species']=='Iris-virginica'], df.PetalLengthCm[df['Species']=='Iris-virginica'] )
ax.scatter(df.SepalLengthCm[df['Species']=='Iris-versicolor'], df.SepalWidthCm[df['Species']=='Iris-versicolor'], df.PetalLengthCm[df['Species']=='Iris-versicolor'] )

ax.set_title('3d Scatter plot')
plt.show()

Dimension Limitation
import seaborn as sns
sns.pairplot(df,hue='Species',size=3,)
x = df.iloc[:, [0, 1, 2, 3]].values

from sklearn.cluster import KMeans
wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', 
                    max_iter = 300, n_init = 10, random_state = 0)
    kmeans.fit(x)
    wcss.append(kmeans.inertia_)
    
# Plotting the results onto a line graph, 
# `allowing us to observe 'The elbow'
plt.plot(range(1, 11), wcss)
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS') # Within cluster sum of squares
plt.show()
# Applying kmeans to the dataset / Creating the kmeans classifier
kmeans = KMeans(n_clusters = 3, init = 'k-means++',
                max_iter = 300, n_init = 10, random_state = 0)
y_kmeans = kmeans.fit_predict(x)
# Visualising the clusters - On the first two columns
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], 
            s = 100, c = 'red', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], 
            s = 100, c = 'blue', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1],
            s = 100, c = 'green', label = 'Iris-virginica')

# Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], 
            s = 100, c = 'yellow', label = 'Centroids')
plt.legend()
